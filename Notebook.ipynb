{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkqTuNpDOACC"
      },
      "source": [
        "## Requirements and setup\n",
        "\n",
        "* Google Colab with access to a GPU.\n",
        "* Data source (Provided)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('github')\n",
        "!git clone https://{github_token}@github.com/starsofchance/Local_MultiMedia_RAG_For_Research.git"
      ],
      "metadata": {
        "id": "brxLAKkDJ5Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAm2CN-4OACC"
      },
      "outputs": [],
      "source": [
        "#cell 1.1\n",
        "# Perform Google Colab installs (if running in Google Colab)\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install pillow # Added for image processing\n",
        "    !pip install qwen-vl-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 1.2\n",
        "# for faster attention mechanism = faster LLM inference\n",
        "# Make sure GPU runtime is enabled\n",
        "!nvidia-smi\n",
        "\n",
        "# Install torch matching Colab CUDA (usually CUDA 12.1)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Then install flash-attn pre-built for PyTorch+CUDA\n",
        "!pip install flash-attn==2.8.3 --no-build-isolation\n"
      ],
      "metadata": {
        "id": "ViE-teKNKKg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import flash_attn\n",
        "    print(f\"✅ flash-attn is installed and importable!\")\n",
        "    print(f\"Version: {flash_attn.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Error importing flash-attn: {e}\")"
      ],
      "metadata": {
        "id": "OLuOVSkpIpco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 2\n",
        "#import your own files if you did not clone the repo.\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create folder\n",
        "folder_name = \"PDF_Files\"\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Upload local files\n",
        "print(\"Select your PDF files from your computer:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move files into PDF_Files/\n",
        "for filename in uploaded.keys():\n",
        "    src = f\"/content/{filename}\"                       # Original auto-upload location\n",
        "    dst = os.path.join(folder_name, filename)          # Desired final location\n",
        "    shutil.move(src, dst)\n",
        "    print(f\"Moved: {dst}\")\n",
        "\n",
        "print(\"\\nAll uploaded files are now ONLY inside the PDF_Files/ folder.\")\n"
      ],
      "metadata": {
        "id": "rOv-Aa4tBb-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [REPLACEMENT] Cell 3: Robust Multimodal Extraction (Raster Images + Vector Charts)\n",
        "# Run this to process ALL files in your PDF_Files folder correctly.\n",
        "\n",
        "import fitz\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def clean_block_text(text: str) -> str:\n",
        "    \"\"\"Cleans text within a specific block.\"\"\"\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "def box_intersect_or_near(box1, box2, threshold=20):\n",
        "    \"\"\"Checks if two bounding boxes intersect or are close.\"\"\"\n",
        "    x_near = not (box1[0] > box2[2] + threshold or box2[0] > box1[2] + threshold)\n",
        "    y_near = not (box1[1] > box2[3] + threshold or box2[1] > box1[3] + threshold)\n",
        "    return x_near and y_near\n",
        "\n",
        "def merge_boxes(boxes):\n",
        "    \"\"\"Merges a list of boxes into one large box.\"\"\"\n",
        "    if not boxes: return None\n",
        "    x0 = min(b[0] for b in boxes)\n",
        "    y0 = min(b[1] for b in boxes)\n",
        "    x1 = max(b[2] for b in boxes)\n",
        "    y1 = max(b[3] for b in boxes)\n",
        "    return fitz.Rect(x0, y0, x1, y1)\n",
        "\n",
        "def cluster_elements(rects, threshold=30):\n",
        "    \"\"\"\n",
        "    Groups nearby vector elements (lines, shapes) into coherent figures.\n",
        "    \"\"\"\n",
        "    if not rects: return []\n",
        "\n",
        "    # Sort by vertical position to optimize clustering\n",
        "    rects.sort(key=lambda r: r[1])\n",
        "\n",
        "    clusters = []\n",
        "    while rects:\n",
        "        current = rects.pop(0)\n",
        "        cluster = [current]\n",
        "\n",
        "        i = 0\n",
        "        while i < len(rects):\n",
        "            candidate = rects[i]\n",
        "            if box_intersect_or_near(current, candidate, threshold):\n",
        "                cluster.append(candidate)\n",
        "                current = merge_boxes([current, candidate])\n",
        "                rects.pop(i)\n",
        "            else:\n",
        "                i += 1\n",
        "        clusters.append(current)\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def open_and_read_pdf(pdf_path: str, filename: str, image_output_dir: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Reads PDF, detects both Images (Pixels) and Drawings (Vector Charts),\n",
        "    renders them, and extracts text.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages_and_texts = []\n",
        "\n",
        "    # Ensure output directory exists (Fixes the FzErrorSystem error)\n",
        "    os.makedirs(image_output_dir, exist_ok=True)\n",
        "\n",
        "    for page_number, page in enumerate(doc):\n",
        "        # 1. Text Extraction\n",
        "        raw_blocks = page.get_text(\"blocks\")\n",
        "        page_text_content = []\n",
        "        for block in raw_blocks:\n",
        "            cleaned = clean_block_text(block[4])\n",
        "            if cleaned:\n",
        "                page_text_content.append(cleaned)\n",
        "        full_page_text = \"\\n\".join(page_text_content)\n",
        "\n",
        "        # 2. Visual Extraction (Images + Drawings)\n",
        "        valid_rects = []\n",
        "\n",
        "        # A. Raster Images (Photos, Icons)\n",
        "        img_info_list = page.get_image_info(xrefs=True)\n",
        "        for img in img_info_list:\n",
        "            if img['xref'] != 0:\n",
        "                valid_rects.append(fitz.Rect(img['bbox']))\n",
        "\n",
        "        # B. Vector Drawings (Charts, Graphs)\n",
        "        drawings = page.get_drawings()\n",
        "        for draw in drawings:\n",
        "            r = draw[\"rect\"]\n",
        "            # Filter noise (dots, page borders)\n",
        "            if r.width < 10 or r.height < 10: continue\n",
        "            if r.width > page.rect.width * 0.95: continue\n",
        "            valid_rects.append(r)\n",
        "\n",
        "        # 3. Cluster and Render\n",
        "        # Group scattered lines/images into single Diagram figures\n",
        "        clustered_figures = cluster_elements(valid_rects, threshold=50)\n",
        "\n",
        "        image_metadata = []\n",
        "        for img_idx, fig_rect in enumerate(clustered_figures):\n",
        "            # Ignore tiny clusters\n",
        "            if fig_rect.width < 60 or fig_rect.height < 60:\n",
        "                continue\n",
        "\n",
        "            # Render the vector area to a high-res PNG\n",
        "            pix = page.get_pixmap(clip=fig_rect, matrix=fitz.Matrix(3.0, 3.0))\n",
        "\n",
        "            image_name = f\"{os.path.splitext(filename)[0]}_p{page_number+1}_fig{img_idx}.png\"\n",
        "            image_save_path = os.path.join(image_output_dir, image_name)\n",
        "\n",
        "            try:\n",
        "                pix.save(image_save_path)\n",
        "                image_metadata.append({\n",
        "                    \"image_path\": image_save_path,\n",
        "                    \"bbox\": [fig_rect.x0, fig_rect.y0, fig_rect.x1, fig_rect.y1],\n",
        "                    \"potential_caption\": None # Can implement caption search if needed\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Failed to save image {image_name}: {e}\")\n",
        "\n",
        "        pages_and_texts.append({\n",
        "            \"filename\": filename,\n",
        "            \"page_number\": page_number + 1,\n",
        "            \"text\": full_page_text,\n",
        "            \"images\": image_metadata,\n",
        "\n",
        "            # Stats for downstream compatibility\n",
        "            \"page_char_count\": len(full_page_text),\n",
        "            \"page_word_count\": len(full_page_text.split()),\n",
        "            \"page_sentence_count_raw\": len(full_page_text.split(\". \")),\n",
        "            \"page_token_count\": len(full_page_text)/4\n",
        "        })\n",
        "\n",
        "    return pages_and_texts\n",
        "\n",
        "# --- MAIN EXECUTION FOR ALL FILES ---\n",
        "pdf_folder = \"PDF_Files\"\n",
        "image_output_folder = \"Extracted_Images\"\n",
        "all_pages_and_texts = []\n",
        "\n",
        "# List all PDFs\n",
        "pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
        "\n",
        "print(f\"[INFO] Processing {len(pdf_files)} PDF files (Vector + Raster extraction)...\")\n",
        "\n",
        "for filename in tqdm(pdf_files):\n",
        "    pdf_path = os.path.join(pdf_folder, filename)\n",
        "    file_data = open_and_read_pdf(pdf_path, filename, image_output_folder)\n",
        "    all_pages_and_texts.extend(file_data)\n",
        "\n",
        "print(f\"\\n[INFO] Processing complete.\")\n",
        "print(f\"Total pages processed: {len(all_pages_and_texts)}\")"
      ],
      "metadata": {
        "id": "n7XMhSd08q5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/Extracted_Images.zip /content/Extracted_Images"
      ],
      "metadata": {
        "id": "p6zUDDCtCeRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, Image as IPImage, Markdown\n",
        "\n",
        "# 1. Global Statistics\n",
        "total_pages = len(all_pages_and_texts)\n",
        "total_images = sum(len(p['images']) for p in all_pages_and_texts)\n",
        "unique_files = set(p['filename'] for p in all_pages_and_texts)\n",
        "\n",
        "print(f\"--- Dataset Statistics ---\")\n",
        "print(f\"Files Processed: {len(unique_files)}\")\n",
        "print(f\"Total Pages:     {total_pages}\")\n",
        "print(f\"Total Images:    {total_images}\")\n",
        "print(f\"Avg Tokens/Page: {sum(p['page_token_count'] for p in all_pages_and_texts) / total_pages:.1f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 2. Find a \"Rich\" Sample (Page with Images)\n",
        "# We specifically filter for a page that has images to verify our multimodal logic\n",
        "pages_with_images = [p for p in all_pages_and_texts if len(p['images']) == 1]\n",
        "\n",
        "if pages_with_images:\n",
        "    sample = random.choice(pages_with_images)\n",
        "    print(f\"\\n[INSPECTION] Metadata for: {sample['filename']} (Page {sample['page_number']})\")\n",
        "\n",
        "    # Show Text Snippet (First 500 chars)\n",
        "    print(f\"\\n--- Text Snippet (First 500 chars) ---\")\n",
        "    print(sample['text'][:500] + \"...\" if len(sample['text']) > 500 else sample['text'])\n",
        "\n",
        "    # Show Image Metadata & Render First Image\n",
        "    print(f\"\\n--- Image Metadata ({len(sample['images'])} found) ---\")\n",
        "    for img in sample['images']:\n",
        "        print(f\"Path: {img['image_path']}\")\n",
        "        print(f\"Caption Candidate: {img['potential_caption']}\")\n",
        "        print(f\"BBox: {img['bbox']}\")\n",
        "\n",
        "        # Display the actual image in Colab\n",
        "        display(IPImage(filename=img['image_path'], width=300))\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"\\n[WARN] No images found in any processed pages.\")\n",
        "\n",
        "# 3. Pandas Overview (Optional - good for spotting outliers)\n",
        "df = pd.DataFrame(all_pages_and_texts)\n",
        "print(\"\\n--- DataFrame Summary (Top 5 rows) ---\")\n",
        "display(df[[\"filename\", \"page_number\", \"page_token_count\", \"images\"]].head())"
      ],
      "metadata": {
        "id": "FmaMHmTSQJtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Smart Inspection - Focus on Graphs & Charts\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "# 1. Global Statistics\n",
        "total_pages = len(all_pages_and_texts)\n",
        "total_images = sum(len(p['images']) for p in all_pages_and_texts)\n",
        "unique_files = set(p['filename'] for p in all_pages_and_texts)\n",
        "\n",
        "print(f\"--- Dataset Statistics ---\")\n",
        "print(f\"Files Processed: {len(unique_files)}\")\n",
        "print(f\"Total Pages:     {total_pages}\")\n",
        "print(f\"Total Images:    {total_images}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 2. Define \"Interesting\" Images (Heuristic: Area > 20,000 pixels)\n",
        "# This filters out small author photos (~50x50) and keeps charts (~300x200+)\n",
        "def get_image_area(bbox):\n",
        "    width = bbox[2] - bbox[0]\n",
        "    height = bbox[3] - bbox[1]\n",
        "    return width * height\n",
        "\n",
        "# Filter pages that have at least one \"Large\" image\n",
        "rich_pages = []\n",
        "for p in all_pages_and_texts:\n",
        "    # Check if page has any image with area > 15,000\n",
        "    if any(get_image_area(img['bbox']) > 15000 for img in p['images']):\n",
        "        rich_pages.append(p)\n",
        "\n",
        "if rich_pages:\n",
        "    # Pick a random page from the \"High Quality\" list\n",
        "    sample = random.choice(rich_pages)\n",
        "    print(f\"\\n[INSPECTION] Metadata for: {sample['filename']} (Page {sample['page_number']})\")\n",
        "\n",
        "    # Show Text Snippet\n",
        "    print(f\"\\n--- Text Snippet ---\")\n",
        "    print(sample['text'][:300] + \"...\" )\n",
        "\n",
        "    print(f\"\\n--- Visuals Found on Page ({len(sample['images'])}) ---\")\n",
        "\n",
        "    # SORT images by size (Largest First) so we see the main Chart, not the footer logo\n",
        "    sorted_images = sorted(sample['images'], key=lambda x: get_image_area(x['bbox']), reverse=True)\n",
        "\n",
        "    for img in sorted_images:\n",
        "        area = get_image_area(img['bbox'])\n",
        "\n",
        "        # Only display if it's not tiny noise\n",
        "        if area > 5000:\n",
        "            print(f\"Path: {img['image_path']}\")\n",
        "            print(f\"Size Score: {area:.0f} (Likely a Chart/Figure)\")\n",
        "            display(IPImage(filename=img['image_path'], width=400))\n",
        "            print(\"-\" * 30)\n",
        "        else:\n",
        "            print(f\"[Skipping small image/icon, size {area:.0f}]\")\n",
        "else:\n",
        "    print(\"\\n[WARN] No pages with large images found. Try lowering the area threshold.\")"
      ],
      "metadata": {
        "id": "j7w6LqUcD2_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [UPDATED] Cell 5: Create chunks and link images based on Page Association\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def split_text_into_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Splits text into overlapping chunks using a simple sliding window.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_len = len(text)\n",
        "\n",
        "    while start < text_len:\n",
        "        end = start + chunk_size\n",
        "        if end < text_len:\n",
        "            lookback = text[end-50:end]\n",
        "            last_space = lookback.rfind(\" \")\n",
        "            if last_space != -1:\n",
        "                end = (end - 50) + last_space\n",
        "\n",
        "        chunk_text = text[start:end].strip()\n",
        "        if len(chunk_text) > 50:\n",
        "            chunks.append({\n",
        "                \"text\": chunk_text,\n",
        "                \"start_char_idx\": start,\n",
        "                \"end_char_idx\": end\n",
        "            })\n",
        "        start += (chunk_size - overlap)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_multimodal_chunks(pages_data: list[dict]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Generates chunks and links images.\n",
        "    Updated Strategy: Since vector chart captions are hard to extract perfectly,\n",
        "    we associate ALL images on a page with ALL text chunks on that page.\n",
        "    This ensures high retrieval recall (the model will always see the charts relevant to the text).\n",
        "    \"\"\"\n",
        "    final_chunks = []\n",
        "\n",
        "    print(\"[INFO] Starting chunking process (Page-Based Association)...\")\n",
        "\n",
        "    for page in tqdm(pages_data):\n",
        "        # 1. Split the page text\n",
        "        raw_chunks = split_text_into_chunks(page[\"text\"])\n",
        "\n",
        "        for chunk in raw_chunks:\n",
        "            chunk_text = chunk[\"text\"]\n",
        "            attached_images = []\n",
        "\n",
        "            # 2. Attach Images\n",
        "            # Strategy: If an image exists on this page, link it to this chunk.\n",
        "            # This is robust for Research Papers where charts are usually relevant to the whole page text.\n",
        "            if page[\"images\"]:\n",
        "                for img in page[\"images\"]:\n",
        "                    attached_images.append(img[\"image_path\"])\n",
        "\n",
        "            # 3. Create Chunk Object with ALL Metadata\n",
        "            final_chunks.append({\n",
        "                \"id\": f\"{page['filename']}_p{page['page_number']}_{chunk['start_char_idx']}\",\n",
        "                \"filename\": page[\"filename\"],\n",
        "                \"page_number\": page[\"page_number\"],\n",
        "\n",
        "                # The Content\n",
        "                \"text\": chunk_text,\n",
        "                \"images\": attached_images, # Link established!\n",
        "\n",
        "                # Chunk-Specific Stats\n",
        "                \"chunk_char_count\": len(chunk_text),\n",
        "                \"chunk_token_count\": len(chunk_text) / 4,\n",
        "\n",
        "                # Original Page Stats (Carried Over)\n",
        "                \"orig_page_char_count\": page[\"page_char_count\"],\n",
        "                \"orig_page_word_count\": page[\"page_word_count\"],\n",
        "                \"orig_page_sentence_count\": page[\"page_sentence_count_raw\"],\n",
        "                \"orig_page_token_count\": page[\"page_token_count\"]\n",
        "            })\n",
        "\n",
        "    print(f\"[INFO] Created {len(final_chunks)} chunks from {len(pages_data)} pages.\")\n",
        "    return final_chunks\n",
        "\n",
        "# Execute Chunking\n",
        "chunks = create_multimodal_chunks(all_pages_and_texts)\n",
        "\n",
        "# Verify the metadata link\n",
        "chunks_with_images = [c for c in chunks if len(c['images']) > 0]\n",
        "print(f\"\\n[INSPECTION] Total Chunks with Images: {len(chunks_with_images)}\")\n",
        "\n",
        "if chunks_with_images:\n",
        "    print(f\"First Linked Chunk Image Count: {len(chunks_with_images[0]['images'])}\")"
      ],
      "metadata": {
        "id": "zdXhPnGRPQxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [UPDATED] Cell 6: DataFrame Statistics & Inspection\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Convert our list of chunk dictionaries to a pandas DataFrame\n",
        "df = pd.DataFrame(chunks)\n",
        "\n",
        "# 1. Inspect the columns\n",
        "# Note: 'embedding' column will NOT be here yet (we generate it in the next step)\n",
        "print(f\"Dataframe Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# 2. Display the first 5 rows\n",
        "display_cols = [\n",
        "    \"filename\",\n",
        "    \"page_number\",\n",
        "    \"text\",\n",
        "    \"images\",\n",
        "    \"chunk_token_count\",\n",
        "    \"orig_page_token_count\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Knowledge Base Preview ---\")\n",
        "display(df[display_cols].head().style.set_properties(subset=['text'], **{'text-align': 'left', 'white-space': 'pre-wrap'}))\n",
        "\n",
        "# 3. Statistical Summary (Tokens & Images)\n",
        "print(\"\\n--- Statistical Summary ---\")\n",
        "\n",
        "# Add a temporary column to count images per chunk for analysis\n",
        "df['image_count'] = df['images'].apply(len)\n",
        "\n",
        "# We analyze:\n",
        "# - chunk_token_count: Size of text inputs\n",
        "# - image_count: Verification of image linking (Should be > 0 max)\n",
        "stats_cols = [\"chunk_token_count\", \"orig_page_token_count\", \"image_count\"]\n",
        "display(df[stats_cols].describe().round(2))"
      ],
      "metadata": {
        "id": "s9hNM4orPuGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the High-Performance Qwen3 Model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 1. Load Model\n",
        "# Qwen3-Embedding-0.6B:\n",
        "# - Score: 64.7 (Excellent)\n",
        "# - Size: ~1.1GB (Leaves room for LLM)\n",
        "# - Context: 32k tokens\n",
        "print(f\"[INFO] Loading embedding model: Qwen/Qwen3-Embedding-0.6B on {device}...\")\n",
        "\n",
        "embedding_model = SentenceTransformer(\n",
        "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
        "    trust_remote_code=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# 2. Embed\n",
        "# Note: For Qwen models, it is often best practice to add a task instruction for queries,\n",
        "# but for *documents* (what we are doing now), we usually embed them raw.\n",
        "print(\"[INFO] Generating embeddings...\")\n",
        "chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
        "\n",
        "# We increase batch_size slightly since this model is efficient\n",
        "embeddings = embedding_model.encode(chunk_texts, batch_size=64, show_progress_bar=True)\n",
        "\n",
        "# 3. Store\n",
        "for i, chunk in enumerate(chunks):\n",
        "    chunk[\"embedding\"] = embeddings[i]\n",
        "\n",
        "# 4. Save to Disk\n",
        "df = pd.DataFrame(chunks)\n",
        "save_path = \"multimodal_rag_embeddings_qwen.pkl\"\n",
        "df.to_pickle(save_path)\n",
        "\n",
        "print(f\"\\n[INFO] Embedding complete.\")\n",
        "print(f\"Total Vectors: {len(chunks)}\")\n",
        "print(f\"Vector Dimension: {len(chunks[0]['embedding'])}\") # Expected: 1024 or 1536 for Qwen (usually larger than mpnet)\n",
        "print(f\"Saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "FK2GympRBLQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"[INFO] Loading Knowledge Base from Pickle...\")\n",
        "\n",
        "# 1. Load the DataFrame (Pickle preserves data types, so no string parsing needed)\n",
        "df = pd.read_pickle(\"multimodal_rag_embeddings_qwen.pkl\")\n",
        "\n",
        "# 2. Convert embeddings to a PyTorch Tensor\n",
        "# Since we used Pickle, the 'embedding' column is already a list of arrays.\n",
        "# We just stack them and convert to Tensor.\n",
        "text_embeddings = torch.tensor(np.stack(df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
        "\n",
        "# 3. Convert metadata to a list of dictionaries (for easy access during retrieval)\n",
        "pages_and_chunks = df.to_dict(orient=\"records\")\n",
        "\n",
        "print(f\"[INFO] Loading complete.\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Embeddings Shape: {text_embeddings.shape}\")\n",
        "# Shape should be [Num_Chunks, Model_Dim] (e.g., [1416, 1024] for Qwen)"
      ],
      "metadata": {
        "id": "V21QFHZmBLOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import textwrap\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# 1. Load the same model used for the database\n",
        "# We need this to convert your questions into vectors\n",
        "print(\"[INFO] Loading Qwen embedding model for retrieval...\")\n",
        "embedding_model = SentenceTransformer(\n",
        "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
        "    trust_remote_code=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def retrieve_relevant_resources(query: str, embeddings: torch.Tensor, n_resources: int = 5):\n",
        "    \"\"\"\n",
        "    Embeds the query and returns the top-k most similar chunks from the database.\n",
        "    \"\"\"\n",
        "    # Embed the query\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate Similarity (Dot Product)\n",
        "    # This compares the query vector to all 1416 chunk vectors at once\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "\n",
        "    # Get top-k results\n",
        "    scores, indices = torch.topk(dot_scores, k=n_resources)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def print_multimodal_results(query, scores, indices):\n",
        "    \"\"\"\n",
        "    Prints text and DISPLAYS IMAGES for retrieved chunks.\n",
        "    \"\"\"\n",
        "    print(f\"\\nQuery: '{query}'\\n\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for score, idx in zip(scores, indices):\n",
        "        chunk_data = pages_and_chunks[idx.item()]\n",
        "\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        print(f\"Source: {chunk_data['filename']} (Page {chunk_data['page_number']})\")\n",
        "        print(\"\\nText:\")\n",
        "        print(textwrap.fill(chunk_data[\"text\"], width=80))\n",
        "\n",
        "        # --- THE MULTIMODAL PART ---\n",
        "        # If this text chunk has an attached image, display it!\n",
        "        if chunk_data[\"images\"]:\n",
        "            print(f\"\\n[Visual Context Found] Displaying {len(chunk_data['images'])} image(s):\")\n",
        "            for img_path in chunk_data[\"images\"]:\n",
        "                display(Image(filename=img_path, width=400))\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# --- Test the Search ---\n",
        "# Try a query relevant to your PDFs (Security/LLMs)\n",
        "query = \"in the paper called:Malware Detection at the Edge with Lightweight LLMs: A Performance Evaluation, the TON-IoT dataset includes what?\"\n",
        "\n",
        "scores, indices = retrieve_relevant_resources(query, text_embeddings)\n",
        "print_multimodal_results(query, scores, indices)"
      ],
      "metadata": {
        "id": "cA1ICbYRBLLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"complete this paragraph:TON-IoT. The TON-IoT dataset, released in 2021 by Moustafa et al. [6], stands as one of\"\n",
        "\n",
        "scores, indices = retrieve_relevant_resources(query, text_embeddings)\n",
        "print_multimodal_results(query, scores, indices)\n"
      ],
      "metadata": {
        "id": "6SwXnFBCHpUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Memory Cleanup & LLM Loading\n",
        "import gc\n",
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "# 1. FORCE CLEANUP\n",
        "# Delete the embedding model and any large intermediate variables\n",
        "if 'embedding_model' in globals():\n",
        "    del embedding_model\n",
        "if 'embeddings' in globals(): # The raw array, we keep text_embeddings tensor\n",
        "    del embeddings\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"[INFO] Memory flushed.\")\n",
        "print(f\"GPU Memory Free: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")\n",
        "\n",
        "# 2. Load the LLM (Qwen2.5-7B-Instruct)\n",
        "# We use 4-bit quantization to fit ~16GB of weights into roughly 5-6GB of VRAM\n",
        "# [NEW] Cell 12: Load Qwen2-VL (Vision Language Model)\n",
        "\n",
        "\n",
        "# 1. Quantization Config (Same as before, keeps it under 16GB)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"[INFO] Loading Qwen2-VL-7B-Instruct (Vision Model)...\")\n",
        "\n",
        "# 2. Load The Vision Model\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 3. Load The Processor (Handles Images + Text)\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", trust_remote_code=True)\n",
        "\n",
        "print(\"[INFO] Vision Model Loaded Successfully!\")"
      ],
      "metadata": {
        "id": "rmnNCC8nBLJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Reload the Embedding Model (Required for Search)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# We check if it's already loaded to avoid reloading unnecessarily\n",
        "if 'embedding_model' not in globals():\n",
        "    print(\"[INFO] Reloading Qwen Embedding Model for Search...\")\n",
        "    embedding_model = SentenceTransformer(\n",
        "        \"Qwen/Qwen3-Embedding-0.6B\",\n",
        "        trust_remote_code=True,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"[INFO] Embedding Model Loaded.\")\n",
        "else:\n",
        "    print(\"[INFO] Embedding Model already active.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7sn1oDbgpxNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nMuMPPvJjHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJznaNSyJjDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BIbgWBZXJi_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################multimdeia model"
      ],
      "metadata": {
        "id": "BYX08cli-cgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Multimodal RAG (Vision-Language Model with Controls)\n",
        "# [UPDATED] Cell 20: Multimodal RAG with Semantic Image Filtering\n",
        "from IPython.display import display, Image as IPImage\n",
        "from sentence_transformers import util\n",
        "\n",
        "def talk_to_rag(query, target_file=None, top_k=25, top_visuals=2, score_threshold=0.35, debug=False):\n",
        "    \"\"\"\n",
        "    Multimodal RAG with Smart Image Filtering.\n",
        "\n",
        "    New Parameter:\n",
        "    - score_threshold: If an image's surrounding text doesn't match the query\n",
        "                       above this score, the image is hidden. (Prevents irrelevant diagrams).\n",
        "                       Default 0.35 is a good balance for Qwen embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    context_text = \"\"\n",
        "    images_to_process = []\n",
        "    sources_used = []\n",
        "\n",
        "    # --- MODE 1: TARGET SPECIFIC FILE (Now with Smart Image Selection) ---\n",
        "    if target_file:\n",
        "        print(f\"[MODE] Reading full document: {target_file}\")\n",
        "\n",
        "        # 1. Get all chunks for this file\n",
        "        file_indices = [i for i, c in enumerate(pages_and_chunks) if c['filename'] == target_file]\n",
        "\n",
        "        if not file_indices:\n",
        "            print(f\"[ERROR] File '{target_file}' not found.\")\n",
        "            return\n",
        "\n",
        "        # 2. Build Full Context (Text is still the WHOLE paper)\n",
        "        # We sort by page number so the LLM reads the paper in order\n",
        "        file_chunks = [pages_and_chunks[i] for i in file_indices]\n",
        "        file_chunks.sort(key=lambda x: x['page_number'])\n",
        "        full_text = \"\\n\\n\".join([c['text'] for c in file_chunks])\n",
        "        context_text = f\"DOCUMENT: {target_file}\\nCONTENT:\\n{full_text}\"\n",
        "\n",
        "        # 3. SMART IMAGE SELECTION\n",
        "        # Instead of taking the first images blindly, we rank the file's chunks by relevance.\n",
        "        if debug: print(f\"[DEBUG] Ranking {len(file_indices)} chunks in file for visual relevance...\")\n",
        "\n",
        "        # Get embeddings just for this file\n",
        "        target_embeddings = text_embeddings[file_indices]\n",
        "        query_vec = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "        # Calculate similarity within this file\n",
        "        scores = util.dot_score(query_vec, target_embeddings)[0]\n",
        "\n",
        "        # Pair up (Score, Chunk)\n",
        "        scored_chunks = []\n",
        "        for i, score in enumerate(scores):\n",
        "            idx = file_indices[i] # Original global index\n",
        "            scored_chunks.append((score.item(), pages_and_chunks[idx]))\n",
        "\n",
        "        # Sort by relevance (Highest Score First)\n",
        "        scored_chunks.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Pick images ONLY from the most relevant parts of the paper\n",
        "        for score, chunk in scored_chunks:\n",
        "            if chunk['images']:\n",
        "                # CHECK THRESHOLD: Is this image actually related?\n",
        "                if score < score_threshold:\n",
        "                    if debug: print(f\"[SKIP] Image on Page {chunk['page_number']} (Score {score:.4f} < {score_threshold})\")\n",
        "                    continue\n",
        "\n",
        "                for img in chunk['images']:\n",
        "                    if img not in images_to_process and len(images_to_process) < top_visuals:\n",
        "                        if debug: print(f\"[KEEP] Image on Page {chunk['page_number']} (Score {score:.4f})\")\n",
        "                        images_to_process.append(img)\n",
        "\n",
        "    # --- MODE 2: SEARCH DATABASE (Standard RAG) ---\n",
        "    else:\n",
        "        print(f\"[MODE] Searching database for: '{query}' (Deep Search: Top {top_k})\")\n",
        "        scores, indices = retrieve_relevant_resources(query, text_embeddings, n_resources=top_k)\n",
        "\n",
        "        context_items = []\n",
        "        if debug: print(\"\\n[DEBUG] Rankings:\")\n",
        "\n",
        "        for rank, idx in enumerate(indices):\n",
        "            chunk = pages_and_chunks[idx.item()]\n",
        "            score = scores[rank].item()\n",
        "\n",
        "            if debug: print(f\"#{rank+1}: {chunk['filename']} (Score: {score:.4f})\")\n",
        "\n",
        "            context_items.append(chunk[\"text\"])\n",
        "\n",
        "            src = f\"{chunk['filename']} (Page {chunk['page_number']})\"\n",
        "            if src not in sources_used: sources_used.append(src)\n",
        "\n",
        "            # Smart Image Selection (Standard Mode)\n",
        "            if chunk[\"images\"]:\n",
        "                # Check threshold here too (Consistency)\n",
        "                if score < score_threshold:\n",
        "                    if debug: print(f\"[SKIP] Image on Page {chunk['page_number']} (Low Score)\")\n",
        "                    continue\n",
        "\n",
        "                for img in chunk[\"images\"]:\n",
        "                    if img not in images_to_process and len(images_to_process) < top_visuals:\n",
        "                        images_to_process.append(img)\n",
        "\n",
        "        context_text = \"\\n\\n---\\n\\n\".join(context_items)\n",
        "\n",
        "    # --- GENERATION ---\n",
        "    if not images_to_process:\n",
        "        print(f\"[INFO] No relevant images found (Threshold: {score_threshold}). Generating text-only response...\")\n",
        "        content_payload = [] # Text only\n",
        "    else:\n",
        "        print(f\"[INFO] Generating answer with {len(images_to_process)} visual inputs...\")\n",
        "        content_payload = []\n",
        "        for img_path in images_to_process:\n",
        "            content_payload.append({\"type\": \"image\", \"image\": img_path})\n",
        "\n",
        "    # Add Text Prompt\n",
        "    system_prompt = f\"\"\"You are an academic assistant.\n",
        "    Answer the user's question using the provided CONTEXT.\n",
        "    If images are provided, use them to support your answer.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context_text[:30000]}\n",
        "    \"\"\"\n",
        "\n",
        "    content_payload.append({\"type\": \"text\", \"text\": f\"{system_prompt}\\n\\nUSER QUERY: {query}\"})\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": content_payload}]\n",
        "\n",
        "    # Process\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, _ = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    answer = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"ANSWER:\\n{answer}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Show user what images were analyzed\n",
        "    if images_to_process:\n",
        "        print(\"\\n[Visuals Analyzed]\")\n",
        "        for img_path in images_to_process:\n",
        "            print(f\"File: {img_path}\")\n",
        "            display(IPImage(filename=img_path, width=400))"
      ],
      "metadata": {
        "id": "0q6se2eG-cdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#target the whole dataset\n",
        "query = \"What does Fig1 in paper called Malware Detection at the Edge with Lightweight LLMs_ A Performance Evaluation shows?.\"\n",
        "\n",
        "talk_to_rag(query, top_k=10, debug=True, top_visuals=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "wfTL6mk7-cak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target an specific paper\n",
        "query = \"the TON-IoT dataset is created rom what? what number of smaples is in it and what is it used for??\"\n",
        "talk_to_rag(query, top_k=10, debug=False, target_file=\"Malware Detection at the Edge with Lightweight LLMs_ A Performance Evaluation.pdf\")\n"
      ],
      "metadata": {
        "id": "CULgxp4Upa89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oIVlD1Xbpa6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxrpIa4apa3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYvZl9QN-cDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKcFfDHy-cAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AFter restarting the env:"
      ],
      "metadata": {
        "id": "NqSq67wo-b9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h10Yz_fD-b6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('github')\n",
        "!git clone https://{github_token}@github.com/starsofchance/Local_MultiMedia_RAG_For_Research.git"
      ],
      "metadata": {
        "id": "moHSIzRLN4nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nocg7qesN4nb"
      },
      "outputs": [],
      "source": [
        "#cell 1.1\n",
        "# Perform Google Colab installs (if running in Google Colab)\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install pillow # Added for image processing\n",
        "    !pip install qwen-vl-utils\n",
        "    #cell 1.2\n",
        "    # for faster attention mechanism = faster LLM inference\n",
        "    # Make sure GPU runtime is enabled\n",
        "    # Install torch matching Colab CUDA (usually CUDA 12.1)\n",
        "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "    # Then install flash-attn pre-built for PyTorch+CUDA\n",
        "    !pip install flash-attn==2.8.3 --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import fitz # PyMuPDF\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "# --- HELPER FUNCTIONS (Must be defined before the Class) ---\n",
        "# (Paste the helper functions 'open_and_read_pdf', 'create_multimodal_chunks', etc. here if they aren't already in the cell)\n",
        "# For brevity, I assume you have the helpers from the previous \"Cell 2\" block.\n",
        "\n",
        "class MultimodalRAG:\n",
        "    def __init__(self, db_path=\"multimodal_rag_embeddings_qwen.pkl\",\n",
        "                 pdf_folder=\"PDF_Files\",\n",
        "                 image_folder=\"Extracted_Images\"):\n",
        "\n",
        "        self.db_path = db_path\n",
        "        self.pdf_folder = pdf_folder\n",
        "        self.image_folder = image_folder\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # State\n",
        "        self.pages_and_chunks = []\n",
        "        self.text_embeddings = None\n",
        "\n",
        "        # Models\n",
        "        self.embedding_model = None\n",
        "        self.vision_model = None\n",
        "        self.processor = None\n",
        "\n",
        "        # Auto-Load DB\n",
        "        self.load_database()\n",
        "\n",
        "    def load_database(self):\n",
        "        \"\"\"Loads the .pkl database into memory.\"\"\"\n",
        "        if os.path.exists(self.db_path):\n",
        "            print(f\"[INIT] Loading database from {self.db_path}...\")\n",
        "            df = pd.read_pickle(self.db_path)\n",
        "            self.pages_and_chunks = df.to_dict(orient=\"records\")\n",
        "\n",
        "            # Convert list of arrays to Tensor\n",
        "            self.text_embeddings = torch.tensor(np.stack(df[\"embedding\"].tolist()), dtype=torch.float32).to(self.device)\n",
        "            print(f\"[SUCCESS] Loaded {len(self.pages_and_chunks)} chunks.\")\n",
        "        else:\n",
        "            print(f\"[INIT] Database not found. Starting fresh.\")\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Loads Qwen Embedding (Search) and Qwen-VL (Vision Brain).\"\"\"\n",
        "        print(\"[LOAD] Loading Embedding Model...\")\n",
        "        self.embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", trust_remote_code=True, device=self.device)\n",
        "\n",
        "        print(\"[LOAD] Loading Vision Model...\")\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "        self.vision_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "            \"Qwen/Qwen2-VL-7B-Instruct\", quantization_config=bnb_config,\n",
        "            device_map=\"auto\", trust_remote_code=True\n",
        "        )\n",
        "        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", trust_remote_code=True)\n",
        "        print(\"[SUCCESS] Models Ready.\")\n",
        "\n",
        "    def ingest_new_files(self):\n",
        "        \"\"\"Scans PDF_Files, processes new PDFs, and updates the DB.\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            print(\"[ERROR] Models not loaded. Run .load_models()\"); return\n",
        "\n",
        "        existing_files = set(c['filename'] for c in self.pages_and_chunks)\n",
        "        if not os.path.exists(self.pdf_folder): os.makedirs(self.pdf_folder)\n",
        "        disk_files = set(f for f in os.listdir(self.pdf_folder) if f.lower().endswith(\".pdf\"))\n",
        "        new_files = list(disk_files - existing_files)\n",
        "\n",
        "        if not new_files:\n",
        "            print(\"[INFO] No new PDF files found.\"); return\n",
        "\n",
        "        print(f\"[UPDATE] Processing {len(new_files)} new files...\")\n",
        "        new_chunks_buffer = []\n",
        "\n",
        "        for filename in tqdm(new_files):\n",
        "            full_path = os.path.join(self.pdf_folder, filename)\n",
        "            # Calls the global helper functions\n",
        "            pages = open_and_read_pdf(full_path, filename, self.image_folder)\n",
        "            chunks = create_multimodal_chunks(pages)\n",
        "\n",
        "            texts = [c[\"text\"] for c in chunks]\n",
        "            vectors = self.embedding_model.encode(texts, batch_size=32, show_progress_bar=False)\n",
        "\n",
        "            for i, chunk in enumerate(chunks): chunk[\"embedding\"] = vectors[i]\n",
        "            new_chunks_buffer.extend(chunks)\n",
        "\n",
        "        if new_chunks_buffer:\n",
        "            self.pages_and_chunks.extend(new_chunks_buffer)\n",
        "            new_tensor = torch.tensor(np.stack([c['embedding'] for c in new_chunks_buffer]), dtype=torch.float32).to(self.device)\n",
        "\n",
        "            if self.text_embeddings is None: self.text_embeddings = new_tensor\n",
        "            else: self.text_embeddings = torch.cat((self.text_embeddings, new_tensor), dim=0)\n",
        "\n",
        "            pd.DataFrame(self.pages_and_chunks).to_pickle(self.db_path)\n",
        "            print(f\"[SUCCESS] Database updated. Total Chunks: {len(self.pages_and_chunks)}\")\n",
        "\n",
        "    def chat(self, query, target_file=None, top_k=25, top_visuals=2, score_threshold=0.35, debug=False):\n",
        "        \"\"\"\n",
        "        Main RAG Interface.\n",
        "        - target_file: If set, restricts context to one paper.\n",
        "        - top_visuals: Max images to show the model.\n",
        "        - score_threshold: Minimum similarity score (0.0-1.0) to consider an image relevant.\n",
        "        \"\"\"\n",
        "        if self.vision_model is None: print(\"[ERROR] Models not loaded.\"); return\n",
        "\n",
        "        context_items = []\n",
        "        images_to_process = []\n",
        "        sources = []\n",
        "\n",
        "        # Embed Query\n",
        "        query_vec = self.embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "        # --- MODE 1: Talk to Specific Paper ---\n",
        "        if target_file:\n",
        "            print(f\"[MODE] Reading document: {target_file}\")\n",
        "            # Find indices for this file\n",
        "            file_indices = [i for i, c in enumerate(self.pages_and_chunks) if c['filename'] == target_file]\n",
        "\n",
        "            if not file_indices: print(f\"[ERROR] File not found.\"); return\n",
        "\n",
        "            # Get subset of embeddings for this file to check relevance\n",
        "            target_embeddings = self.text_embeddings[file_indices]\n",
        "            scores = util.dot_score(query_vec, target_embeddings)[0]\n",
        "\n",
        "            # Sort chunks by relevance\n",
        "            scored_chunks = []\n",
        "            for i, score in enumerate(scores):\n",
        "                idx = file_indices[i]\n",
        "                scored_chunks.append((score.item(), self.pages_and_chunks[idx]))\n",
        "\n",
        "            # Sort chunks by Page Number for reading flow (Text)\n",
        "            # But we use the Scores for Image Filtering\n",
        "            file_chunks_sorted = sorted([c for _, c in scored_chunks], key=lambda x: x['page_number'])\n",
        "\n",
        "            # Build Text Context (Whole Paper)\n",
        "            context_items = [c['text'] for c in file_chunks_sorted]\n",
        "            sources.append(f\"Full Document: {target_file}\")\n",
        "\n",
        "            # Select Images (Based on Score)\n",
        "            scored_chunks.sort(key=lambda x: x[0], reverse=True) # Sort by score for images\n",
        "\n",
        "            for score, chunk in scored_chunks:\n",
        "                if chunk['images']:\n",
        "                    if score < score_threshold:\n",
        "                        if debug: print(f\"[SKIP] Img on Page {chunk['page_number']} (Score {score:.2f} < {score_threshold})\")\n",
        "                        continue\n",
        "\n",
        "                    for img in chunk['images']:\n",
        "                        if img not in images_to_process and len(images_to_process) < top_visuals:\n",
        "                            images_to_process.append(img)\n",
        "\n",
        "        # --- MODE 2: Standard Search ---\n",
        "        else:\n",
        "            print(f\"[MODE] Searching DB (Top {top_k})...\")\n",
        "            scores = util.dot_score(query_vec, self.text_embeddings)[0]\n",
        "            top_scores, top_indices = torch.topk(scores, k=top_k)\n",
        "\n",
        "            for rank, idx in enumerate(top_indices):\n",
        "                chunk = self.pages_and_chunks[idx.item()]\n",
        "                score = top_scores[rank].item()\n",
        "\n",
        "                if debug: print(f\"#{rank+1}: {chunk['filename']} ({score:.4f})\")\n",
        "\n",
        "                context_items.append(chunk[\"text\"])\n",
        "                src = f\"{chunk['filename']} (Page {chunk['page_number']})\"\n",
        "                if src not in sources: sources.append(src)\n",
        "\n",
        "                # Filter Images\n",
        "                if chunk[\"images\"]:\n",
        "                    if score < score_threshold: continue\n",
        "                    for img in chunk[\"images\"]:\n",
        "                        if img not in images_to_process and len(images_to_process) < top_visuals:\n",
        "                            images_to_process.append(img)\n",
        "\n",
        "        # --- GENERATION ---\n",
        "        msg = f\"[AI] Thinking with {len(images_to_process)} images...\"\n",
        "        if len(images_to_process) == 0 and score_threshold > 0: msg += \" (others filtered by threshold)\"\n",
        "        print(msg)\n",
        "\n",
        "        content_payload = []\n",
        "        for img in images_to_process:\n",
        "            content_payload.append({\"type\": \"image\", \"image\": img})\n",
        "\n",
        "        sys_prompt = f\"\"\"You are an academic assistant.\n",
        "        Answer the user's question using the provided CONTEXT.\n",
        "        If images are provided, use them to support your answer.\n",
        "        CONTEXT: {' '.join(context_items)[:32000]}\"\"\"\n",
        "\n",
        "        content_payload.append({\"type\": \"text\", \"text\": f\"{sys_prompt}\\n\\nQUERY: {query}\"})\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": content_payload}]\n",
        "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        image_inputs, _ = process_vision_info(messages)\n",
        "        inputs = self.processor(text=[text], images=image_inputs, padding=True, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        gen_ids = self.vision_model.generate(**inputs, max_new_tokens=1024)\n",
        "        answer = self.processor.batch_decode(gen_ids, skip_special_tokens=True)[0].split(\"<|im_start|>assistant\\n\")[-1].strip()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*40 + f\"\\nANSWER:\\n{answer}\\n\" + \"=\"*40)\n",
        "\n",
        "        if images_to_process:\n",
        "            print(\"\\n[Visuals Analyzed]\")\n",
        "            for img in images_to_process: display(IPImage(filename=img, width=300))"
      ],
      "metadata": {
        "id": "mwF7ymGCNj_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Initialize (The Start Button)\n",
        "\n",
        "# Run this to wake everything up.\n",
        "rag = MultimodalRAG()\n",
        "rag.load_models()"
      ],
      "metadata": {
        "id": "ctaUsAm7Nj83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mode 1: Search the Whole Database (Default)\n",
        "\n",
        "Use this when you want to compare papers or find answers from \"somewhere\" in your data.\n",
        "Python\n",
        "\n",
        "# Query only\n",
        "rag.chat(\"What is the difference between data poisoning and prompt injection?\")\n",
        "\n",
        "Mode 2: Talk to One Specific Paper\n",
        "\n",
        "Use this when you want to summarize a file or ask about a specific figure in a specific file. Note: You must copy the filename exactly (including .pdf).\n",
        "Python\n",
        "\n",
        "# Query + target_file\n",
        "rag.chat(\"Summarize the methodology of this paper\", target_file=\"My_Paper.pdf\")\n",
        "\n",
        "Bonus: Search + Visual Control\n",
        "\n",
        "You can combine search with the visual limit (e.g., look at 4 charts).\n",
        "Python\n",
        "\n",
        "rag.chat(\"Compare the accuracy charts\", top_visuals=4)"
      ],
      "metadata": {
        "id": "cx5fZuiQSgOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Talk\n",
        "rag.chat(\"what is the goal in Prompt injection attacks on vision language models in oncology? what does the attacers trying to achive?\")"
      ],
      "metadata": {
        "id": "o1rTnkSPQI4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag.chat(\"What is the difference between data poisoning and prompt injection?\")"
      ],
      "metadata": {
        "id": "H9t_IZlLNj6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag.chat(\"explain the METHODOLOGY in the paper\",\n",
        "         target_file=\"BadPre Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models.pdf\",\n",
        "         top_k=10)"
      ],
      "metadata": {
        "id": "siRERUdfNj3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TIldy3aYNj0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DzIVTFoONjxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfHSXwoGNjuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zy-DGnU0NjrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6oK7GhwNjnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vhdd96hQewM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NaFI4t-qewKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxMjS9wAewH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nozmH7yFewFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yhwDGKAtcHcf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}